{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import pickle\n",
    "import time\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data to improve data! There is not enough variance hence cant seperate classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"recipe_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_name = []\n",
    "heading_steps = []\n",
    "heading_ingr = []\n",
    "for i in range(100):\n",
    "    heading_name.append(\"name\"+str(i))\n",
    "    heading_steps.append(\"steps\"+str(i))\n",
    "    heading_ingr.append(\"ingr\"+str(i))\n",
    "    \n",
    "recipe_train = pd.read_csv('recipe_train.csv')\n",
    "n_steps = recipe_train['n_steps']\n",
    "n_ingr = recipe_train['n_ingredients']\n",
    "duration_label = recipe_train['duration_label']\n",
    "\n",
    "name = pd.read_csv('recipe_text_features_doc2vec100/train_name_doc2vec100.csv', names=heading_name)\n",
    "steps = pd.read_csv('recipe_text_features_doc2vec100/train_steps_doc2vec100.csv', names=heading_steps)\n",
    "ingr = pd.read_csv('recipe_text_features_doc2vec100/train_ingr_doc2vec100.csv', names=heading_ingr)\n",
    "\n",
    "data = name.join(steps).join(ingr).join(n_steps).join(n_ingr)\n",
    "labeled_data = name.join(steps).join(ingr).join(n_steps).join(n_ingr).join(duration_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name0</th>\n",
       "      <th>name1</th>\n",
       "      <th>name2</th>\n",
       "      <th>name3</th>\n",
       "      <th>name4</th>\n",
       "      <th>name5</th>\n",
       "      <th>name6</th>\n",
       "      <th>name7</th>\n",
       "      <th>name8</th>\n",
       "      <th>name9</th>\n",
       "      <th>...</th>\n",
       "      <th>ingr92</th>\n",
       "      <th>ingr93</th>\n",
       "      <th>ingr94</th>\n",
       "      <th>ingr95</th>\n",
       "      <th>ingr96</th>\n",
       "      <th>ingr97</th>\n",
       "      <th>ingr98</th>\n",
       "      <th>ingr99</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>n_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.138903</td>\n",
       "      <td>-0.105632</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.184168</td>\n",
       "      <td>-0.060962</td>\n",
       "      <td>0.158132</td>\n",
       "      <td>0.183160</td>\n",
       "      <td>-0.047310</td>\n",
       "      <td>-0.094243</td>\n",
       "      <td>-0.190823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250643</td>\n",
       "      <td>0.214695</td>\n",
       "      <td>-0.018045</td>\n",
       "      <td>0.395938</td>\n",
       "      <td>-0.057779</td>\n",
       "      <td>0.336876</td>\n",
       "      <td>0.230647</td>\n",
       "      <td>0.143468</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050503</td>\n",
       "      <td>-0.009516</td>\n",
       "      <td>0.103751</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>-0.077112</td>\n",
       "      <td>0.064507</td>\n",
       "      <td>-0.077452</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>0.011587</td>\n",
       "      <td>-0.273610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122143</td>\n",
       "      <td>-0.188637</td>\n",
       "      <td>0.096245</td>\n",
       "      <td>0.296148</td>\n",
       "      <td>0.098384</td>\n",
       "      <td>-0.124963</td>\n",
       "      <td>-0.195799</td>\n",
       "      <td>-0.210086</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017546</td>\n",
       "      <td>-0.065912</td>\n",
       "      <td>0.018216</td>\n",
       "      <td>-0.136358</td>\n",
       "      <td>-0.157555</td>\n",
       "      <td>0.012741</td>\n",
       "      <td>-0.090726</td>\n",
       "      <td>-0.172800</td>\n",
       "      <td>-0.013132</td>\n",
       "      <td>-0.139782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>-0.271923</td>\n",
       "      <td>-0.188931</td>\n",
       "      <td>-0.154470</td>\n",
       "      <td>-0.174869</td>\n",
       "      <td>-0.341245</td>\n",
       "      <td>-0.060525</td>\n",
       "      <td>0.080168</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.192465</td>\n",
       "      <td>0.100614</td>\n",
       "      <td>-0.135092</td>\n",
       "      <td>-0.045592</td>\n",
       "      <td>-0.084184</td>\n",
       "      <td>0.011720</td>\n",
       "      <td>-0.273193</td>\n",
       "      <td>-0.112106</td>\n",
       "      <td>-0.342789</td>\n",
       "      <td>-0.048324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259047</td>\n",
       "      <td>0.107682</td>\n",
       "      <td>-0.259195</td>\n",
       "      <td>-0.065767</td>\n",
       "      <td>-0.065660</td>\n",
       "      <td>-0.207306</td>\n",
       "      <td>-0.167152</td>\n",
       "      <td>0.492161</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.294862</td>\n",
       "      <td>0.072540</td>\n",
       "      <td>-0.079729</td>\n",
       "      <td>-0.123215</td>\n",
       "      <td>-0.006412</td>\n",
       "      <td>-0.029505</td>\n",
       "      <td>0.014645</td>\n",
       "      <td>0.058601</td>\n",
       "      <td>0.113420</td>\n",
       "      <td>-0.282019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131887</td>\n",
       "      <td>0.215795</td>\n",
       "      <td>0.119587</td>\n",
       "      <td>-0.063841</td>\n",
       "      <td>-0.107087</td>\n",
       "      <td>-0.281038</td>\n",
       "      <td>-0.136156</td>\n",
       "      <td>0.063560</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>0.079531</td>\n",
       "      <td>-0.152311</td>\n",
       "      <td>-0.108301</td>\n",
       "      <td>0.245858</td>\n",
       "      <td>0.027275</td>\n",
       "      <td>0.123829</td>\n",
       "      <td>0.170607</td>\n",
       "      <td>-0.097053</td>\n",
       "      <td>-0.195209</td>\n",
       "      <td>-0.161817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110938</td>\n",
       "      <td>0.007777</td>\n",
       "      <td>-0.021177</td>\n",
       "      <td>0.016590</td>\n",
       "      <td>-0.189764</td>\n",
       "      <td>-0.068398</td>\n",
       "      <td>-0.190228</td>\n",
       "      <td>0.083059</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>0.175133</td>\n",
       "      <td>-0.062132</td>\n",
       "      <td>-0.037974</td>\n",
       "      <td>-0.036284</td>\n",
       "      <td>-0.065828</td>\n",
       "      <td>0.023028</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.003984</td>\n",
       "      <td>-0.072117</td>\n",
       "      <td>-0.204766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019811</td>\n",
       "      <td>0.015345</td>\n",
       "      <td>0.050862</td>\n",
       "      <td>0.087614</td>\n",
       "      <td>0.594146</td>\n",
       "      <td>0.507868</td>\n",
       "      <td>0.109975</td>\n",
       "      <td>-0.047240</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>0.056546</td>\n",
       "      <td>0.125747</td>\n",
       "      <td>-0.161598</td>\n",
       "      <td>0.088675</td>\n",
       "      <td>-0.130160</td>\n",
       "      <td>-0.099027</td>\n",
       "      <td>-0.086974</td>\n",
       "      <td>-0.129478</td>\n",
       "      <td>-0.043179</td>\n",
       "      <td>-0.026247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111844</td>\n",
       "      <td>0.219837</td>\n",
       "      <td>0.034625</td>\n",
       "      <td>0.033108</td>\n",
       "      <td>-0.518219</td>\n",
       "      <td>0.229315</td>\n",
       "      <td>-0.024256</td>\n",
       "      <td>-0.351173</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>-0.001341</td>\n",
       "      <td>0.042503</td>\n",
       "      <td>0.026502</td>\n",
       "      <td>0.187291</td>\n",
       "      <td>0.063780</td>\n",
       "      <td>0.078871</td>\n",
       "      <td>-0.180524</td>\n",
       "      <td>-0.104264</td>\n",
       "      <td>0.017374</td>\n",
       "      <td>-0.337967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157026</td>\n",
       "      <td>-0.158611</td>\n",
       "      <td>0.375092</td>\n",
       "      <td>0.225296</td>\n",
       "      <td>-0.409948</td>\n",
       "      <td>-0.642923</td>\n",
       "      <td>0.512482</td>\n",
       "      <td>-0.925799</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>0.121345</td>\n",
       "      <td>-0.045553</td>\n",
       "      <td>-0.015056</td>\n",
       "      <td>-0.097100</td>\n",
       "      <td>0.141332</td>\n",
       "      <td>0.199203</td>\n",
       "      <td>0.098336</td>\n",
       "      <td>-0.064405</td>\n",
       "      <td>0.263377</td>\n",
       "      <td>-0.305463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137772</td>\n",
       "      <td>0.195448</td>\n",
       "      <td>-0.206056</td>\n",
       "      <td>0.092833</td>\n",
       "      <td>-0.045537</td>\n",
       "      <td>-0.035035</td>\n",
       "      <td>-0.058196</td>\n",
       "      <td>0.035302</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          name0     name1     name2     name3     name4     name5     name6  \\\n",
       "0     -0.138903 -0.105632  0.211538  0.184168 -0.060962  0.158132  0.183160   \n",
       "1      0.050503 -0.009516  0.103751  0.001047 -0.077112  0.064507 -0.077452   \n",
       "2      0.017546 -0.065912  0.018216 -0.136358 -0.157555  0.012741 -0.090726   \n",
       "3     -0.192465  0.100614 -0.135092 -0.045592 -0.084184  0.011720 -0.273193   \n",
       "4      0.294862  0.072540 -0.079729 -0.123215 -0.006412 -0.029505  0.014645   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "39995  0.079531 -0.152311 -0.108301  0.245858  0.027275  0.123829  0.170607   \n",
       "39996  0.175133 -0.062132 -0.037974 -0.036284 -0.065828  0.023028 -0.064448   \n",
       "39997  0.056546  0.125747 -0.161598  0.088675 -0.130160 -0.099027 -0.086974   \n",
       "39998 -0.001341  0.042503  0.026502  0.187291  0.063780  0.078871 -0.180524   \n",
       "39999  0.121345 -0.045553 -0.015056 -0.097100  0.141332  0.199203  0.098336   \n",
       "\n",
       "          name7     name8     name9  ...    ingr92    ingr93    ingr94  \\\n",
       "0     -0.047310 -0.094243 -0.190823  ... -0.250643  0.214695 -0.018045   \n",
       "1      0.008811  0.011587 -0.273610  ... -0.122143 -0.188637  0.096245   \n",
       "2     -0.172800 -0.013132 -0.139782  ...  0.127930 -0.271923 -0.188931   \n",
       "3     -0.112106 -0.342789 -0.048324  ... -0.259047  0.107682 -0.259195   \n",
       "4      0.058601  0.113420 -0.282019  ...  0.131887  0.215795  0.119587   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "39995 -0.097053 -0.195209 -0.161817  ... -0.110938  0.007777 -0.021177   \n",
       "39996 -0.003984 -0.072117 -0.204766  ... -0.019811  0.015345  0.050862   \n",
       "39997 -0.129478 -0.043179 -0.026247  ...  0.111844  0.219837  0.034625   \n",
       "39998 -0.104264  0.017374 -0.337967  ... -0.157026 -0.158611  0.375092   \n",
       "39999 -0.064405  0.263377 -0.305463  ...  0.137772  0.195448 -0.206056   \n",
       "\n",
       "         ingr95    ingr96    ingr97    ingr98    ingr99  n_steps  \\\n",
       "0      0.395938 -0.057779  0.336876  0.230647  0.143468        6   \n",
       "1      0.296148  0.098384 -0.124963 -0.195799 -0.210086        9   \n",
       "2     -0.154470 -0.174869 -0.341245 -0.060525  0.080168       15   \n",
       "3     -0.065767 -0.065660 -0.207306 -0.167152  0.492161       10   \n",
       "4     -0.063841 -0.107087 -0.281038 -0.136156  0.063560        6   \n",
       "...         ...       ...       ...       ...       ...      ...   \n",
       "39995  0.016590 -0.189764 -0.068398 -0.190228  0.083059        6   \n",
       "39996  0.087614  0.594146  0.507868  0.109975 -0.047240       15   \n",
       "39997  0.033108 -0.518219  0.229315 -0.024256 -0.351173        5   \n",
       "39998  0.225296 -0.409948 -0.642923  0.512482 -0.925799        7   \n",
       "39999  0.092833 -0.045537 -0.035035 -0.058196  0.035302        6   \n",
       "\n",
       "       n_ingredients  \n",
       "0                 12  \n",
       "1                  5  \n",
       "2                 10  \n",
       "3                  8  \n",
       "4                  5  \n",
       "...              ...  \n",
       "39995             13  \n",
       "39996             16  \n",
       "39997              8  \n",
       "39998             17  \n",
       "39999             11  \n",
       "\n",
       "[40000 rows x 302 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgbc = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(xgbc, data, duration_label, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data, duration_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:53:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "203.76141595840454 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "xgbc.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.717"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_eta1 = xgb.XGBClassifier(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "xgbc_eta1.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgbc_eta1.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_eta2 = xgb.XGBClassifier(learning_rate = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "224.28854656219482\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "xgbc_eta2.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7185"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_eta2.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94578125"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_eta2.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_colsam1 = xgb.XGBClassifier(learning_rate = 0.2, colsample_bytree = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:58:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "44.88545870780945 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "xgbc_colsam1.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.717625"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_colsam1.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93796875"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_colsam1.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_range = np.arange(0.1, 0.35, 0.05)\n",
    "max_depth_range = np.arange(3,16,1)\n",
    "min_child_weight_range = np.arange(1,9,2)\n",
    "gamma_range = np.arange(0, 0.5, 0.1)\n",
    "colsample_bytree_range = np.arange(0.3, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with feature n_steps and n_ingredients only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(data[['n_steps', 'n_ingredients']], duration_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "1.355980396270752 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "xgbc_eta2.fit(X_train2, Y_train2)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.636375"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_eta2.score(X_test2, Y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc = LogisticRegression(solver = 'saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.13733434677124 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lrc.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7275"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.728337049484253 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lrc1.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725625"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc1.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc2 = LogisticRegression(max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.66108751296997 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lrc2.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.728"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc2.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc3 = LogisticRegression(max_iter = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.8420729637146 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lrc3.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.728125"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc3.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for overfiiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7348125"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc3.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test classifier on iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = load_iris()\n",
    "\n",
    "cross_val_score(xgbc, x.data, x.target, cv=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
