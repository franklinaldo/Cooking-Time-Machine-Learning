{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt"
    "import time\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing count vectorizer files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franklinaldo/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.3 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Count vectorizer\n",
    "vocab = pickle.load(open(\"recipe_text_features_countvec/train_name_countvectorizer.pkl\", \"rb\"))\n"
    "# Preprocess the data to improve data! There is not enough variance hence cant seperate classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = vocab.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "spares_recipe_name = scipy.sparse.load_npz('recipe_text_features_countvec/train_name_vec.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_name = spares_recipe.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4801"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict['irish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 799\n",
      "1 1653\n",
      "1 3479\n",
      "1 5605\n",
      "1 8698\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(recipe_name[0])):\n",
    "    if recipe_name[10][i]!=0:\n",
    "        print(recipe_name[10][i], end=\" \")\n",
    "        print(i)\n"
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Doc2vec files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_name = []\n",
    "heading_steps = []\n",
    "heading_ingr = []\n",
    "for i in range(100):\n",
    "    heading_name.append(\"name\"+str(i))\n",
    "    heading_steps.append(\"steps\"+str(i))\n",
    "    heading_ingr.append(\"ingr\"+str(i))\n",
    "\n",
    "heading_name_50 = heading_name[:50]\n",
    "heading_steps_50 = heading_steps[:50]\n",
    "heading_ingr_50 = heading_ingr[:50]\n",
    "\n",
    "recipe_train = pd.read_csv('recipe_train.csv')\n",
    "n_steps = recipe_train['n_steps']\n",
    "n_ingr = recipe_train['n_ingredients']\n",
    "duration_label = recipe_train['duration_label']\n",
    "\n",
    "name_100 = pd.read_csv('recipe_text_features_doc2vec100/train_name_doc2vec100.csv', names=heading_name)\n",
    "steps_100 = pd.read_csv('recipe_text_features_doc2vec100/train_steps_doc2vec100.csv', names=heading_steps)\n",
    "ingr_100 = pd.read_csv('recipe_text_features_doc2vec100/train_ingr_doc2vec100.csv', names=heading_ingr)\n",
    "\n",
    "name_50 = pd.read_csv('recipe_text_features_doc2vec50/train_name_doc2vec50.csv', names=heading_name_50)\n",
    "steps_50 = pd.read_csv('recipe_text_features_doc2vec50/train_steps_doc2vec50.csv', names=heading_steps_50)\n",
    "ingr_50 = pd.read_csv('recipe_text_features_doc2vec50/train_ingr_doc2vec50.csv', names=heading_ingr_50)\n",
    "\n",
    "data_100 = name_100.join(steps_100).join(ingr_100).join(n_steps).join(n_ingr)\n",
    "data_50 = name_50.join(steps_50).join(ingr_50).join(n_steps).join(n_ingr)"
    "data = name.join(steps).join(ingr).join(n_steps).join(n_ingr)\n",
    "labeled_data = name.join(steps).join(ingr).join(n_steps).join(n_ingr).join(duration_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name0</th>\n",
       "      <th>name1</th>\n",
       "      <th>name2</th>\n",
       "      <th>name3</th>\n",
       "      <th>name4</th>\n",
       "      <th>name5</th>\n",
       "      <th>name6</th>\n",
       "      <th>name7</th>\n",
       "      <th>name8</th>\n",
       "      <th>name9</th>\n",
       "      <th>...</th>\n",
       "      <th>name90</th>\n",
       "      <th>name91</th>\n",
       "      <th>name92</th>\n",
       "      <th>name93</th>\n",
       "      <th>name94</th>\n",
       "      <th>name95</th>\n",
       "      <th>name96</th>\n",
       "      <th>name97</th>\n",
       "      <th>name98</th>\n",
       "      <th>name99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.138903</td>\n",
       "      <td>-0.105632</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.184168</td>\n",
       "      <td>-0.060962</td>\n",
       "      <td>0.158132</td>\n",
       "      <td>0.183160</td>\n",
       "      <td>-0.047310</td>\n",
       "      <td>-0.094243</td>\n",
       "      <td>-0.190823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207824</td>\n",
       "      <td>0.244199</td>\n",
       "      <td>-0.037218</td>\n",
       "      <td>-0.075407</td>\n",
       "      <td>0.048488</td>\n",
       "      <td>-0.050525</td>\n",
       "      <td>0.024342</td>\n",
       "      <td>-0.446743</td>\n",
       "      <td>0.161369</td>\n",
       "      <td>-0.099595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.050503</td>\n",
       "      <td>-0.009516</td>\n",
       "      <td>0.103751</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>-0.077112</td>\n",
       "      <td>0.064507</td>\n",
       "      <td>-0.077452</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>0.011587</td>\n",
       "      <td>-0.273610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070235</td>\n",
       "      <td>0.112170</td>\n",
       "      <td>0.107049</td>\n",
       "      <td>0.087395</td>\n",
       "      <td>-0.125569</td>\n",
       "      <td>-0.145412</td>\n",
       "      <td>0.046910</td>\n",
       "      <td>-0.139394</td>\n",
       "      <td>0.248779</td>\n",
       "      <td>0.013672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017546</td>\n",
       "      <td>-0.065912</td>\n",
       "      <td>0.018216</td>\n",
       "      <td>-0.136358</td>\n",
       "      <td>-0.157555</td>\n",
       "      <td>0.012741</td>\n",
       "      <td>-0.090726</td>\n",
       "      <td>-0.172800</td>\n",
       "      <td>-0.013132</td>\n",
       "      <td>-0.139782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043418</td>\n",
       "      <td>-0.015630</td>\n",
       "      <td>-0.021166</td>\n",
       "      <td>0.183753</td>\n",
       "      <td>0.072158</td>\n",
       "      <td>0.227696</td>\n",
       "      <td>0.030992</td>\n",
       "      <td>-0.255895</td>\n",
       "      <td>0.051713</td>\n",
       "      <td>0.027578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.192465</td>\n",
       "      <td>0.100614</td>\n",
       "      <td>-0.135092</td>\n",
       "      <td>-0.045592</td>\n",
       "      <td>-0.084184</td>\n",
       "      <td>0.011720</td>\n",
       "      <td>-0.273193</td>\n",
       "      <td>-0.112106</td>\n",
       "      <td>-0.342789</td>\n",
       "      <td>-0.048324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155918</td>\n",
       "      <td>-0.089216</td>\n",
       "      <td>-0.261898</td>\n",
       "      <td>0.078533</td>\n",
       "      <td>-0.091570</td>\n",
       "      <td>-0.078378</td>\n",
       "      <td>0.058694</td>\n",
       "      <td>0.032701</td>\n",
       "      <td>0.219217</td>\n",
       "      <td>-0.002026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.294862</td>\n",
       "      <td>0.072540</td>\n",
       "      <td>-0.079729</td>\n",
       "      <td>-0.123215</td>\n",
       "      <td>-0.006412</td>\n",
       "      <td>-0.029505</td>\n",
       "      <td>0.014645</td>\n",
       "      <td>0.058601</td>\n",
       "      <td>0.113420</td>\n",
       "      <td>-0.282019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065010</td>\n",
       "      <td>-0.132902</td>\n",
       "      <td>-0.070257</td>\n",
       "      <td>-0.006231</td>\n",
       "      <td>0.081670</td>\n",
       "      <td>0.019120</td>\n",
       "      <td>-0.025063</td>\n",
       "      <td>0.124917</td>\n",
       "      <td>0.033979</td>\n",
       "      <td>-0.070665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>0.079531</td>\n",
       "      <td>-0.152311</td>\n",
       "      <td>-0.108301</td>\n",
       "      <td>0.245858</td>\n",
       "      <td>0.027275</td>\n",
       "      <td>0.123829</td>\n",
       "      <td>0.170607</td>\n",
       "      <td>-0.097053</td>\n",
       "      <td>-0.195209</td>\n",
       "      <td>-0.161817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102784</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>0.035881</td>\n",
       "      <td>0.048117</td>\n",
       "      <td>-0.062391</td>\n",
       "      <td>-0.126580</td>\n",
       "      <td>0.066401</td>\n",
       "      <td>0.021117</td>\n",
       "      <td>0.096193</td>\n",
       "      <td>-0.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>0.175133</td>\n",
       "      <td>-0.062132</td>\n",
       "      <td>-0.037974</td>\n",
       "      <td>-0.036284</td>\n",
       "      <td>-0.065828</td>\n",
       "      <td>0.023028</td>\n",
       "      <td>-0.064448</td>\n",
       "      <td>-0.003984</td>\n",
       "      <td>-0.072117</td>\n",
       "      <td>-0.204766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077884</td>\n",
       "      <td>0.141417</td>\n",
       "      <td>0.064914</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.008758</td>\n",
       "      <td>-0.072497</td>\n",
       "      <td>-0.108400</td>\n",
       "      <td>-0.162673</td>\n",
       "      <td>0.130834</td>\n",
       "      <td>-0.081396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>0.056546</td>\n",
       "      <td>0.125747</td>\n",
       "      <td>-0.161598</td>\n",
       "      <td>0.088675</td>\n",
       "      <td>-0.130160</td>\n",
       "      <td>-0.099027</td>\n",
       "      <td>-0.086974</td>\n",
       "      <td>-0.129478</td>\n",
       "      <td>-0.043179</td>\n",
       "      <td>-0.026247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034453</td>\n",
       "      <td>0.095062</td>\n",
       "      <td>0.083830</td>\n",
       "      <td>0.049767</td>\n",
       "      <td>-0.114336</td>\n",
       "      <td>-0.053334</td>\n",
       "      <td>0.009475</td>\n",
       "      <td>-0.121661</td>\n",
       "      <td>0.014591</td>\n",
       "      <td>-0.108426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>-0.001341</td>\n",
       "      <td>0.042503</td>\n",
       "      <td>0.026502</td>\n",
       "      <td>0.187291</td>\n",
       "      <td>0.063780</td>\n",
       "      <td>0.078871</td>\n",
       "      <td>-0.180524</td>\n",
       "      <td>-0.104264</td>\n",
       "      <td>0.017374</td>\n",
       "      <td>-0.337967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005647</td>\n",
       "      <td>0.043866</td>\n",
       "      <td>-0.244220</td>\n",
       "      <td>0.114175</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.066957</td>\n",
       "      <td>-0.029916</td>\n",
       "      <td>0.040780</td>\n",
       "      <td>-0.113538</td>\n",
       "      <td>-0.095814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>0.121345</td>\n",
       "      <td>-0.045553</td>\n",
       "      <td>-0.015056</td>\n",
       "      <td>-0.097100</td>\n",
       "      <td>0.141332</td>\n",
       "      <td>0.199203</td>\n",
       "      <td>0.098336</td>\n",
       "      <td>-0.064405</td>\n",
       "      <td>0.263377</td>\n",
       "      <td>-0.305463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340563</td>\n",
       "      <td>0.231314</td>\n",
       "      <td>0.290843</td>\n",
       "      <td>-0.382511</td>\n",
       "      <td>-0.357382</td>\n",
       "      <td>-0.243063</td>\n",
       "      <td>-0.090108</td>\n",
       "      <td>-0.153246</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>-0.243836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          name0     name1     name2     name3     name4     name5     name6  \\\n",
       "0     -0.138903 -0.105632  0.211538  0.184168 -0.060962  0.158132  0.183160   \n",
       "1      0.050503 -0.009516  0.103751  0.001047 -0.077112  0.064507 -0.077452   \n",
       "2      0.017546 -0.065912  0.018216 -0.136358 -0.157555  0.012741 -0.090726   \n",
       "3     -0.192465  0.100614 -0.135092 -0.045592 -0.084184  0.011720 -0.273193   \n",
       "4      0.294862  0.072540 -0.079729 -0.123215 -0.006412 -0.029505  0.014645   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "39995  0.079531 -0.152311 -0.108301  0.245858  0.027275  0.123829  0.170607   \n",
       "39996  0.175133 -0.062132 -0.037974 -0.036284 -0.065828  0.023028 -0.064448   \n",
       "39997  0.056546  0.125747 -0.161598  0.088675 -0.130160 -0.099027 -0.086974   \n",
       "39998 -0.001341  0.042503  0.026502  0.187291  0.063780  0.078871 -0.180524   \n",
       "39999  0.121345 -0.045553 -0.015056 -0.097100  0.141332  0.199203  0.098336   \n",
       "\n",
       "          name7     name8     name9  ...    name90    name91    name92  \\\n",
       "0     -0.047310 -0.094243 -0.190823  ... -0.207824  0.244199 -0.037218   \n",
       "1      0.008811  0.011587 -0.273610  ...  0.070235  0.112170  0.107049   \n",
       "2     -0.172800 -0.013132 -0.139782  ... -0.043418 -0.015630 -0.021166   \n",
       "3     -0.112106 -0.342789 -0.048324  ... -0.155918 -0.089216 -0.261898   \n",
       "4      0.058601  0.113420 -0.282019  ...  0.065010 -0.132902 -0.070257   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "39995 -0.097053 -0.195209 -0.161817  ...  0.102784  0.013425  0.035881   \n",
       "39996 -0.003984 -0.072117 -0.204766  ... -0.077884  0.141417  0.064914   \n",
       "39997 -0.129478 -0.043179 -0.026247  ... -0.034453  0.095062  0.083830   \n",
       "39998 -0.104264  0.017374 -0.337967  ... -0.005647  0.043866 -0.244220   \n",
       "39999 -0.064405  0.263377 -0.305463  ...  0.340563  0.231314  0.290843   \n",
       "\n",
       "         name93    name94    name95    name96    name97    name98    name99  \n",
       "0     -0.075407  0.048488 -0.050525  0.024342 -0.446743  0.161369 -0.099595  \n",
       "1      0.087395 -0.125569 -0.145412  0.046910 -0.139394  0.248779  0.013672  \n",
       "2      0.183753  0.072158  0.227696  0.030992 -0.255895  0.051713  0.027578  \n",
       "3      0.078533 -0.091570 -0.078378  0.058694  0.032701  0.219217 -0.002026  \n",
       "4     -0.006231  0.081670  0.019120 -0.025063  0.124917  0.033979 -0.070665  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "39995  0.048117 -0.062391 -0.126580  0.066401  0.021117  0.096193 -0.049300  \n",
       "39996  0.001344  0.008758 -0.072497 -0.108400 -0.162673  0.130834 -0.081396  \n",
       "39997  0.049767 -0.114336 -0.053334  0.009475 -0.121661  0.014591 -0.108426  \n",
       "39998  0.114175  0.004448  0.066957 -0.029916  0.040780 -0.113538 -0.095814  \n",
       "39999 -0.382511 -0.357382 -0.243063 -0.090108 -0.153246  0.000255 -0.243836  \n",
       "\n",
       "[40000 rows x 100 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72018899, 0.72076802, 0.72706818])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(SVC(gamma='auto'), data, duration_label,cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgbc = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(xgbc, data, duration_label, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data, duration_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:53:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "203.76141595840454 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "xgbc.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.717"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_eta1 = xgb.XGBClassifier(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "xgbc_eta1.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgbc_eta1.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_eta2 = xgb.XGBClassifier(learning_rate = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "224.28854656219482\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "xgbc_eta2.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7185"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_eta2.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94578125"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_eta2.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_colsam1 = xgb.XGBClassifier(learning_rate = 0.2, colsample_bytree = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:58:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "44.88545870780945 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "xgbc_colsam1.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.717625"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_colsam1.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93796875"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_colsam1.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_range = np.arange(0.1, 0.35, 0.05)\n",
    "max_depth_range = np.arange(3,16,1)\n",
    "min_child_weight_range = np.arange(1,9,2)\n",
    "gamma_range = np.arange(0, 0.5, 0.1)\n",
    "colsample_bytree_range = np.arange(0.3, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with feature n_steps and n_ingredients only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(data[['n_steps', 'n_ingredients']], duration_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:57:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "1.355980396270752 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "xgbc_eta2.fit(X_train2, Y_train2)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.636375"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc_eta2.score(X_test2, Y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc = LogisticRegression(solver = 'saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.13733434677124 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lrc.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7275"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.728337049484253 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lrc1.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725625"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc1.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc2 = LogisticRegression(max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.66108751296997 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lrc2.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.728"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc2.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc3 = LogisticRegression(max_iter = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.8420729637146 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lrc3.fit(X_train, Y_train)\n",
    "\n",
    "fin = time.time()\n",
    "\n",
    "print(fin-start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.728125"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc3.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for overfiiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7348125"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrc3.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test classifier on iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = load_iris()\n",
    "\n",
    "cross_val_score(xgbc, x.data, x.target, cv=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
